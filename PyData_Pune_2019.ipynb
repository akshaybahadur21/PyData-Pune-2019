{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyData Pune 2019.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03wfFUTLTPUq",
        "colab_type": "text"
      },
      "source": [
        "# Haptic Learning : inferencing human features using deep networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqWekWUnU28p",
        "colab_type": "text"
      },
      "source": [
        "This python notebook is for explanation of the core concepts used and the models developed for this webinar.\n",
        "\n",
        "## Acknowledgement\n",
        "\n",
        "I would like to extend my gratitude towards PyData, Pune team for giving me this opportunity to showcase my findings \n",
        "\n",
        "## Akshay Bahadur\n",
        "\n",
        "    * Software engineer, Symantec.\n",
        "    * ML Researcher\n",
        "    * Software Innovator, Intel\n",
        "    \n",
        "### Contact\n",
        "\n",
        "   * [Portfolio](https://www.akshaybahadur.com/)\n",
        "   * [LinkedIN](https://www.linkedin.com/in/akshaybahadur21/)\n",
        "   * [GitHub](https://github.com/akshaybahadur21)\n",
        "  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgKXg6pdYUBw",
        "colab_type": "text"
      },
      "source": [
        "## Tania's story"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mY-SZTuJSfh-",
        "colab_type": "code",
        "outputId": "7e10d19b-4781-449c-d7c5-b9bcff661350",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        }
      },
      "source": [
        "\n",
        "%%HTML\n",
        "<iframe width=\"700\" height=\"400\" src=\"https://www.youtube.com/embed/Oc_QMQ4QHcw\"></iframe>"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<iframe width=\"700\" height=\"400\" src=\"https://www.youtube.com/embed/Oc_QMQ4QHcw\"></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCz9Nd12ZAhG",
        "colab_type": "text"
      },
      "source": [
        "## MNIST Digit Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIpt43xjCmxx",
        "colab_type": "text"
      },
      "source": [
        "## Showing content using Webcam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9d5xMvDcY0f2",
        "colab_type": "code",
        "outputId": "708401db-8b1a-433e-b3ac-bbfd582cbdf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "\n",
        "from keras import Sequential\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import Flatten, Dense, Dropout\n",
        "from keras.utils import np_utils, print_summary\n",
        "from keras.models import load_model\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_POvOyzZHr8",
        "colab_type": "code",
        "outputId": "ae7ff5ad-ac5a-4c75-feb9-0232266f0126",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfT0D9NBZOZI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def showData(x, label):\n",
        "    pixels = np.array(x, dtype='uint8')\n",
        "\n",
        "    pixels = pixels.reshape((28, 28))\n",
        "\n",
        "    plt.title('Label is {label}'.format(label=label))\n",
        "    plt.imshow(pixels, cmap='gray')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L24QTXK2ZQq1",
        "colab_type": "code",
        "outputId": "b1817e22-bc81-4e12-98c3-a7cf4c554154",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "showData(x_train[250], y_train[250])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaSnNsLKZUXv",
        "colab_type": "code",
        "outputId": "ec03a6b9-97e1-4a45-ffe7-7a76c7eab0a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "showData(x_train[24], y_train[24])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAD39JREFUeJzt3XuMXPV5xvHniS+iNtSxgS4WNnEA\nU8lqgSCLS2WlRGkSgloBf9TYKKpLKEulIBqUVgXaCtqSClkFSoUU1RSDA8EQYjCWayWhiIYUVIqx\nuNhAggEDNjYGO2AoFjb47R9zNlrMzpnZuZ3Zfb8fabSz5z1nzrujffbcZs/PESEA+Xym6gYAVIPw\nA0kRfiApwg8kRfiBpAg/kBThT8L2f9n+s04va/sq2//eXneoAuEfY2xvsf0HVfcxJCL+KSJG/UfF\n9qW219v+0PbtXWgNDUysugGk9YakayV9TdJvVNxLSmz5xwnb022vtf2W7V8Vz2cdNNtxtv/X9h7b\nD9ieMWz5020/Zvsd20/bPrPJ9V5j+87i+SG277S9q3idJ2wPjLRcRNwXEasl7WrxR0abCP/48RlJ\nt0n6nKRjJO2VdPNB8/yJpG9KminpI0n/Kkm2j5b0H6ptiWdI+ktJq2wfOcoelkiaJmm2pMMl/XnR\nB/oQ4R8nImJXRKyKiA8i4j1J35X0+wfNdkdEbIyI/5P0d5IW2p4g6RuS1kXEuog4EBEPSlov6exR\ntrFftdAfHxEfR8STEbGnvZ8M3UL4xwnbU2z/m+1Xbe+R9IikzxbhHvL6sOevSpok6QjV9hb+uNhV\nf8f2O5IWqLaHMBp3SPqJpLttv2F7qe1JLf9Q6CrCP358R9JvSzotIn5T0heL6R42z+xhz49RbUv9\ntmp/FO6IiM8Oe0yNiOtG00BE7I+Iv4+IeZJ+T9IfqnaogT5E+MemScXJtaHHREmHqXZ8/U5xIu/q\nEZb7hu15tqdI+gdJP4qIjyXdKemPbH/N9oTiNc8c4YRhKdtfsv27xd7GHtX+uByoM+9E24dImiBp\nwrCfAz1C+MemdaoFfehxjaR/Ue2S2duS/kfSj0dY7g5Jt0vaIekQSZdJUkS8LukcSVdJeku1PYG/\n0uh/P46S9CPVgv+8pJ8V6xzJ3xa9X6HaOYe9xTT0iLmZB5ATW34gKcIPJEX4gaQIP5BUTy+t2Obs\nItBlEeHGc7W55bd9lu1f2N5s+4p2XgtAb7V8qa/4IMcvJX1F0lZJT0haHBHPlSzDlh/osl5s+U+V\ntDkiXo6IfZLuVu2DIgDGgHbCf7Q++Y8iW4tpn2B7sLhjy/o21gWgw7p+wi8ilklaJrHbD/STdrb8\n2/TJ/xKbVUwDMAa0E/4nJM21/XnbkyUtkrSmM20B6LaWd/sj4iPbl6p284YJkpZHxKaOdQagq3r6\nX30c8wPd15MP+QAYuwg/kBThB5Ii/EBShB9IivADSRF+ICnCDyRF+IGkCD+QFOEHkiL8QFKEH0iK\nUVHRlpUrV5bWTz/99Lq1RYsWlS77+OOPt9QTmsOWH0iK8ANJEX4gKcIPJEX4gaQIP5AU4QeS4u69\naMtjjz1WWj/jjDPq1jZv3ly67Lx580rr+/fvL61nxd17AZQi/EBShB9IivADSRF+ICnCDyRF+IGk\nuM6PUrNnzy6tv/TSS6X1SZMmtbzuKVOmlNb37t3b8muPZ81e52/rZh62t0h6T9LHkj6KiPntvB6A\n3unEnXy+FBFvd+B1APQQx/xAUu2GPyT91PaTtgdHmsH2oO31tte3uS4AHdTubv+CiNhm+7ckPWj7\nhYh4ZPgMEbFM0jKJE35AP2lryx8R24qvOyXdL+nUTjQFoPtaDr/tqbYPG3ou6auSNnaqMQDd1c5u\n/4Ck+20Pvc5dEfHjjnSFvjFt2rTSejvX8VevXl1a//DDD1t+bTTWcvgj4mVJJ3WwFwA9xKU+ICnC\nDyRF+IGkCD+QFOEHkmKI7uQmTiz/Fbjyyiu7tu677rqrtH7gwIGurRts+YG0CD+QFOEHkiL8QFKE\nH0iK8ANJEX4gKa7zJ3fjjTeW1i+44IIedYJeY8sPJEX4gaQIP5AU4QeSIvxAUoQfSIrwA0lxnX+c\nu/jii0vrF110UY86Qb9hyw8kRfiBpAg/kBThB5Ii/EBShB9IivADSXGdfxy48MIL69Zuvvnm0mUn\nT55cWt+wYUNp/ZRTTimto3813PLbXm57p+2Nw6bNsP2g7ReLr9O72yaATmtmt/92SWcdNO0KSQ9F\nxFxJDxXfAxhDGoY/Ih6RtPugyedIWlE8XyHp3A73BaDLWj3mH4iI7cXzHZIG6s1oe1DSYIvrAdAl\nbZ/wi4iwHSX1ZZKWSVLZfAB6q9VLfW/anilJxdednWsJQC+0Gv41kpYUz5dIeqAz7QDolYa7/bZX\nSjpT0hG2t0q6WtJ1kn5o+yJJr0pa2M0m+8Ghhx5at3bSSSeVLnvCCSeU1k877bTS+sKF5W/v9Omt\nX2m97LLLSuvr1q0rrW/evLnldaNaDcMfEYvrlL7c4V4A9BAf7wWSIvxAUoQfSIrwA0kRfiAp/qW3\nSbNmzapbW758eemyjS71NfLuu++W1m+55Za6taVLl5Yuu2XLltJ62c+NsY0tP5AU4QeSIvxAUoQf\nSIrwA0kRfiApwg8kxXX+Jr3wwgt1ayeeeGLpsnPnzm1r3Xv27Cmtv/baa229flWmTp1adQupseUH\nkiL8QFKEH0iK8ANJEX4gKcIPJEX4gaQc0btBdBixZ+w5/PDDS+sbN24srR911FF1a6tXry5d9rzz\nziutY2QR4WbmY8sPJEX4gaQIP5AU4QeSIvxAUoQfSIrwA0nx//wotWvXrtL6K6+8Ulovu87/8MMP\nt9QTOqPhlt/2cts7bW8cNu0a29tsP1U8zu5umwA6rZnd/tslnTXC9Bsj4uTisa6zbQHotobhj4hH\nJO3uQS8AeqidE36X2n6mOCyYXm8m24O219te38a6AHRYq+H/nqTjJJ0sabuk6+vNGBHLImJ+RMxv\ncV0AuqCl8EfEmxHxcUQckHSLpFM72xaAbmsp/LZnDvv2PEnl/9cJoO80vM5ve6WkMyUdYXurpKsl\nnWn7ZEkhaYukS7rYI8ap7du3V91Cag3DHxGLR5h8axd6AdBDfLwXSIrwA0kRfiApwg8kRfiBpPiX\nXnRV2a3hd+7c2cNOcDC2/EBShB9IivADSRF+ICnCDyRF+IGkCD+QFNf5x4Djjz++tD5jxoyWX/uD\nDz4ore/eXX77xhtuuKG0vnTp0rq1I488snTZRvUpU6aU1q+99tq6tXvvvbd02TVr1pTWxwO2/EBS\nhB9IivADSRF+ICnCDyRF+IGkCD+QFNf5O2Dy5Mml9WOPPba0Pjg4WFq/5JLyO6M3ut5dZt++faX1\n999/v7TezmcMGl1rf+utt0rrjd73adOm1a3t2LGjdFmu8wMYtwg/kBThB5Ii/EBShB9IivADSRF+\nIKlmhuieLen7kgZUG5J7WUTcZHuGpHskzVFtmO6FEfGr7rVarYGBgbq1m266qXTZ888/v9PtNK3R\nMNhl99WXpE2bNpXWn3766VH31A9WrFhRdQuVa2bL/5Gk70TEPEmnS/qW7XmSrpD0UETMlfRQ8T2A\nMaJh+CNie0RsKJ6/J+l5SUdLOkfS0J/PFZLO7VaTADpvVMf8tudI+oKkxyUNRMTQPuUO1Q4LAIwR\nTX+23/ahklZJ+nZE7LH961pEhO0RDx5tD0oq//A6gJ5rastve5Jqwf9BRNxXTH7T9syiPlPSiKMu\nRsSyiJgfEfM70TCAzmgYftc28bdKej4iht+qdY2kJcXzJZIe6Hx7ALrFjS712F4g6eeSnpV0oJh8\nlWrH/T+UdIykV1W71Fd6n+d6hwZjweWXX1631uj21e1au3Ztaf3666+vW3v00UdLl92/f39LPaF/\nRYQbz9XEMX9E/Lekei/25dE0BaB/8Ak/ICnCDyRF+IGkCD+QFOEHkiL8QFINr/N3dGVj+Dr/nDlz\n6tYa3eb5jTfeKK3fc889pfXbbruttA4M1+x1frb8QFKEH0iK8ANJEX4gKcIPJEX4gaQIP5AU1/mB\ncYbr/ABKEX4gKcIPJEX4gaQIP5AU4QeSIvxAUoQfSIrwA0kRfiApwg8kRfiBpAg/kBThB5Ii/EBS\nDcNve7bth20/Z3uT7b8opl9je5vtp4rH2d1vF0CnNLyZh+2ZkmZGxAbbh0l6UtK5khZKej8i/rnp\nlXEzD6Drmr2Zx8QmXmi7pO3F8/dsPy/p6PbaA1C1UR3z254j6QuSHi8mXWr7GdvLbU+vs8yg7fW2\n17fVKYCOavoefrYPlfQzSd+NiPtsD0h6W1JI+kfVDg2+2eA12O0HuqzZ3f6mwm97kqS1kn4SETeM\nUJ8jaW1E/E6D1yH8QJd17Aaeti3pVknPDw9+cSJwyHmSNo62SQDVaeZs/wJJP5f0rKQDxeSrJC2W\ndLJqu/1bJF1SnBwsey22/ECXdXS3v1MIP9B93LcfQCnCDyRF+IGkCD+QFOEHkiL8QFKEH0iK8ANJ\nEX4gKcIPJEX4gaQIP5AU4QeSIvxAUg1v4Nlhb0t6ddj3RxTT+lG/9tavfUn01qpO9va5Zmfs6f/z\nf2rl9vqImF9ZAyX6tbd+7Uuit1ZV1Ru7/UBShB9IqurwL6t4/WX6tbd+7Uuit1ZV0lulx/wAqlP1\nlh9ARQg/kFQl4bd9lu1f2N5s+4oqeqjH9hbbzxbDjlc6vmAxBuJO2xuHTZth+0HbLxZfRxwjsaLe\n+mLY9pJh5St97/ptuPueH/PbniDpl5K+ImmrpCckLY6I53raSB22t0iaHxGVfyDE9hclvS/p+0ND\nodleKml3RFxX/OGcHhF/3Se9XaNRDtvepd7qDSv/p6rwvevkcPedUMWW/1RJmyPi5YjYJ+luSedU\n0Effi4hHJO0+aPI5klYUz1eo9svTc3V66wsRsT0iNhTP35M0NKx8pe9dSV+VqCL8R0t6fdj3W1Xh\nGzCCkPRT20/aHqy6mREMDBsWbYekgSqbGUHDYdt76aBh5fvmvWtluPtO44Tfpy2IiFMkfV3St4rd\n274UtWO2frpW+z1Jx6k2huN2SddX2UwxrPwqSd+OiD3Da1W+dyP0Vcn7VkX4t0maPez7WcW0vhAR\n24qvOyXdr9phSj95c2iE5OLrzor7+bWIeDMiPo6IA5JuUYXvXTGs/CpJP4iI+4rJlb93I/VV1ftW\nRfifkDTX9udtT5a0SNKaCvr4FNtTixMxsj1V0lfVf0OPr5G0pHi+RNIDFfbyCf0ybHu9YeVV8XvX\nd8PdR0TPH5LOVu2M/0uS/qaKHur0daykp4vHpqp7k7RStd3A/aqdG7lI0uGSHpL0oqT/lDSjj3q7\nQ7Wh3J9RLWgzK+ptgWq79M9Ieqp4nF31e1fSVyXvGx/vBZLihB+QFOEHkiL8QFKEH0iK8ANJEX4g\nKcIPJPX/oSzieB0xGAsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyap0mSKZWmN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_norm= x_train / 255.\n",
        "x_test_norm=x_test / 255."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ww8EFCNCaA0z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_labels(y):\n",
        "    labels = np_utils.to_categorical(y)\n",
        "    return labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6iPzk3gaGS8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = preprocess_labels(y_train)\n",
        "y_test = preprocess_labels(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oITHCe3aJx2",
        "colab_type": "code",
        "outputId": "b4be55a6-412f-40d9-e1fa-3ee90c848699",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "x_train_norm = x_train_norm.reshape(x_train_norm.shape[0], 28, 28, 1)\n",
        "x_test_norm = x_test_norm.reshape(x_test_norm.shape[0], 28, 28, 1)\n",
        "print(\"number of training examples = \" + str(x_train.shape[0]))\n",
        "print(\"number of test examples = \" + str(x_test.shape[0]))\n",
        "print(\"X_train shape: \" + str(x_train.shape))\n",
        "print(\"Y_train shape: \" + str(y_train.shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of training examples = 60000\n",
            "number of test examples = 10000\n",
            "X_train shape: (60000, 28, 28)\n",
            "Y_train shape: (60000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bof0jyPFaTHr",
        "colab_type": "code",
        "outputId": "a77e523e-080e-4689-8b27-866f692599ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "source": [
        "def keras_model(image_x, image_y):\n",
        "    num_of_classes = 10\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=(image_x, image_y, 1)))\n",
        "    model.add(Dense(512, activation='relu'))\n",
        "    model.add(Dropout(0.6))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.6))\n",
        "    model.add(Dense(num_of_classes, activation='softmax'))\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    filepath = \"pyData.h5\"\n",
        "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "    callbacks_list = [checkpoint]\n",
        "\n",
        "    return model, callbacks_list\n",
        "\n",
        "model, callbacks_list = keras_model(28, 28)\n",
        "print_summary(model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0629 11:49:09.064770 139659769898880 nn_ops.py:4224] Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_3 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 512)               401920    \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 468,874\n",
            "Trainable params: 468,874\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lh_YmtYvaevU",
        "colab_type": "code",
        "outputId": "70a58a78-42b2-4d6c-e587-8304806475c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "model, callbacks_list = keras_model(28, 28)\n",
        "model.fit(x_train_norm, y_train, validation_data=(x_test_norm, y_test), epochs=2, batch_size=64,\n",
        "              callbacks=callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/2\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.4815 - acc: 0.8516 - val_loss: 0.1509 - val_acc: 0.9556\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.95560, saving model to pyData.h5\n",
            "Epoch 2/2\n",
            "60000/60000 [==============================] - 4s 63us/step - loss: 0.2433 - acc: 0.9302 - val_loss: 0.1228 - val_acc: 0.9640\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.95560 to 0.96400, saving model to pyData.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f046c0540b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDfmd0kWajG_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Computer vision part\n",
        "import cv2\n",
        "\n",
        "loaded_model=load_model('pyData.h5')\n",
        "cap = cv2.VideoCapture(0)\n",
        "while (cap.isOpened()):\n",
        "        ret, img = cap.read()\n",
        "        img, contours, thresh = get_img_contour_thresh(img)\n",
        "        if len(contours) > 0:\n",
        "            contour = max(contours, key=cv2.contourArea)\n",
        "            if cv2.contourArea(contour) > 2500:\n",
        "                x, y, w, h = cv2.boundingRect(contour)\n",
        "                newImage = thresh[y:y + h, x:x + w]\n",
        "                newImage = cv2.resize(newImage, (28, 28))\n",
        "                newImage = np.array(newImage)\n",
        "                newImage = newImage.flatten()\n",
        "                newImage = newImage.reshape(newImage.shape[0], 1)\n",
        "                ans= loaded_model.predict(newImage)\n",
        "\n",
        "        x, y, w, h = 0, 0, 300, 300\n",
        "        cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "        cv2.putText(img, \"Prediction : \" + str(ans), (10, 320),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
        "        \n",
        "        cv2.imshow(\"Frame\", img)\n",
        "        cv2.imshow(\"Contours\", thresh)\n",
        "        k = cv2.waitKey(10)\n",
        "        if k == 27:\n",
        "            break\n",
        "\n",
        "\n",
        "def get_img_contour_thresh(img):\n",
        "    x, y, w, h = 0, 0, 300, 300\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    blur = cv2.GaussianBlur(gray, (35, 35), 0)\n",
        "    ret, thresh1 = cv2.threshold(blur, 70, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
        "    thresh1 = thresh1[y:y + h, x:x + w]\n",
        "    contours, hierarchy = cv2.findContours(thresh1, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)[-2:]\n",
        "    return img, contours, thresh1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXEG0fj8D4tF",
        "colab_type": "code",
        "outputId": "56e53062-a643-4958-8c15-0a17bb8019a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        }
      },
      "source": [
        "%%HTML\n",
        "<iframe width=\"700\" height=\"400\" src=\"https://www.youtube.com/embed/MRNODXrYK3Q\"></iframe>"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<iframe width=\"700\" height=\"400\" src=\"https://www.youtube.com/embed/MRNODXrYK3Q\"></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGo4xnpXFyWS",
        "colab_type": "text"
      },
      "source": [
        "# Quick, Draw\n",
        "## Feeding data by writing on screen\n",
        "### For the initial steps, you can look here : https://www.akshaybahadur.com/post/quick-draw\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDnGqbJxEvQ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import deque\n",
        "\n",
        "cap = cv2.VideoCapture(0)\n",
        "Lower_blue = np.array([110, 50, 50])\n",
        "Upper_blue = np.array([130, 255, 255])\n",
        "pts = deque(maxlen=512)\n",
        "blackboard = np.zeros((480, 640, 3), dtype=np.uint8)\n",
        "digit = np.zeros((200, 200, 3), dtype=np.uint8)\n",
        "pred_class = 0\n",
        "\n",
        "model = load_model('QuickDraw.h5')\n",
        "\n",
        "\n",
        "while (cap.isOpened()):\n",
        "        ret, img = cap.read()\n",
        "        img = cv2.flip(img, 1)\n",
        "        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "        kernel = np.ones((5, 5), np.uint8)\n",
        "        mask = cv2.inRange(hsv, Lower_green, Upper_green)\n",
        "        mask = cv2.erode(mask, kernel, iterations=2)\n",
        "        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
        "        mask = cv2.dilate(mask, kernel, iterations=1)\n",
        "        res = cv2.bitwise_and(img, img, mask=mask)\n",
        "        cnts, heir = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[-2:]\n",
        "        center = None\n",
        "\n",
        "        if len(cnts) >= 1:\n",
        "            cnt = max(cnts, key=cv2.contourArea)\n",
        "            if cv2.contourArea(cnt) > 200:\n",
        "                ((x, y), radius) = cv2.minEnclosingCircle(cnt)\n",
        "                cv2.circle(img, (int(x), int(y)), int(radius), (0, 255, 255), 2)\n",
        "                cv2.circle(img, center, 5, (0, 0, 255), -1)\n",
        "                M = cv2.moments(cnt)\n",
        "                center = (int(M['m10'] / M['m00']), int(M['m01'] / M['m00']))\n",
        "                pts.appendleft(center)\n",
        "                for i in range(1, len(pts)):\n",
        "                    if pts[i - 1] is None or pts[i] is None:\n",
        "                        continue\n",
        "                    cv2.line(blackboard, pts[i - 1], pts[i], (255, 255, 255), 7)\n",
        "                    cv2.line(img, pts[i - 1], pts[i], (0, 0, 255), 2)\n",
        "        elif len(cnts) == 0:\n",
        "            if len(pts) != []:\n",
        "                blackboard_gray = cv2.cvtColor(blackboard, cv2.COLOR_BGR2GRAY)\n",
        "                blur1 = cv2.medianBlur(blackboard_gray, 15)\n",
        "                blur1 = cv2.GaussianBlur(blur1, (5, 5), 0)\n",
        "                thresh1 = cv2.threshold(blur1, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
        "                blackboard_cnts = cv2.findContours(thresh1.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)[1]\n",
        "                if len(blackboard_cnts) >= 1:\n",
        "                    cnt = max(blackboard_cnts, key=cv2.contourArea)\n",
        "                    print(cv2.contourArea(cnt))\n",
        "                    if cv2.contourArea(cnt) > 2000:\n",
        "                        x, y, w, h = cv2.boundingRect(cnt)\n",
        "                        digit = blackboard_gray[y:y + h, x:x + w]\n",
        "                        pred_probab, pred_class = keras_predict(model, digit)\n",
        "                        print(pred_class, pred_probab)\n",
        "\n",
        "            pts = deque(maxlen=512)\n",
        "            blackboard = np.zeros((480, 640, 3), dtype=np.uint8)\n",
        "            img = overlay(img, emojis[pred_class], 400, 250, 100, 100)\n",
        "        cv2.imshow(\"Frame\", img)\n",
        "        k = cv2.waitKey(10)\n",
        "        if k == 27:\n",
        "            break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zzx0UwbhPGjb",
        "colab_type": "code",
        "outputId": "c544f8d3-a673-42c7-ae7a-6cb97aaa4ebe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        }
      },
      "source": [
        "%%HTML\n",
        "<iframe width=\"700\" height=\"400\" src=\"https://www.youtube.com/embed/X0qk4aEqg3o\"></iframe>"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<iframe width=\"700\" height=\"400\" src=\"https://www.youtube.com/embed/X0qk4aEqg3o\"></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLQ8YbmTVUdo",
        "colab_type": "code",
        "outputId": "b32baded-18f1-4aa0-af2b-a169edcc628c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        }
      },
      "source": [
        "%%HTML\n",
        "<iframe width=\"700\" height=\"400\" src=\"https://www.youtube.com/embed/Qkpgv16-JRM\"></iframe>"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<iframe width=\"700\" height=\"400\" src=\"https://www.youtube.com/embed/Qkpgv16-JRM\"></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUcEHIG6QO4e",
        "colab_type": "text"
      },
      "source": [
        "# Emojinator\n",
        "## Haptically feeding hand gestures\n",
        "### For more details, you can look here : https://github.com/akshaybahadur21/Emojinator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sU9rA5dPRie",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = load_model('emojinator.h5')\n",
        "\n",
        "cap = cv2.VideoCapture(0)\n",
        "x, y, w, h = 300, 50, 350, 350\n",
        "\n",
        "while (cap.isOpened()):\n",
        "    ret, img = cap.read()\n",
        "    img = cv2.flip(img, 1)\n",
        "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "    mask2 = cv2.inRange(hsv, np.array([2, 50, 60]), np.array([25, 150, 255]))\n",
        "    res = cv2.bitwise_and(img, img, mask=mask2)\n",
        "    gray = cv2.cvtColor(res, cv2.COLOR_BGR2GRAY)\n",
        "    median = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "\n",
        "    kernel_square = np.ones((5, 5), np.uint8)\n",
        "    dilation = cv2.dilate(median, kernel_square, iterations=2)\n",
        "    opening = cv2.morphologyEx(dilation, cv2.MORPH_CLOSE, kernel_square)\n",
        "    ret, thresh = cv2.threshold(opening, 30, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    thresh = thresh[y:y + h, x:x + w]\n",
        "    contours = cv2.findContours(thresh.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)[1]\n",
        "    if len(contours) > 0:\n",
        "        contour = max(contours, key=cv2.contourArea)\n",
        "        if cv2.contourArea(contour) > 2500:\n",
        "            x, y, w1, h1 = cv2.boundingRect(contour)\n",
        "            newImage = thresh[y:y + h1, x:x + w1]\n",
        "            newImage = cv2.resize(newImage, (50, 50))\n",
        "            pred_probab, pred_class = keras_predict(model, newImage)\n",
        "            print(pred_class, pred_probab)\n",
        "            img = overlay(img, emojis[pred_class], 400, 250, 90, 90)\n",
        "\n",
        "    x, y, w, h = 300, 50, 350, 350\n",
        "    cv2.imshow(\"Frame\", img)\n",
        "    cv2.imshow(\"Contours\", thresh)\n",
        "    k = cv2.waitKey(10)\n",
        "    if k == 27:\n",
        "      break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYhwe1-oRvl6",
        "colab_type": "code",
        "outputId": "9dca9a44-35d4-4cb0-a9a1-54122ecfb2ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        }
      },
      "source": [
        "%%HTML\n",
        "<iframe width=\"700\" height=\"400\" src=\"https://www.youtube.com/embed/1eor41gIbF8\"></iframe>"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<iframe width=\"700\" height=\"400\" src=\"https://www.youtube.com/embed/1eor41gIbF8\"></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzSSIKERSGtp",
        "colab_type": "code",
        "outputId": "76a98a5e-b298-4dd5-b531-0cc4cb9085f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        }
      },
      "source": [
        "%%HTML\n",
        "<iframe width=\"700\" height=\"400\" src=\"https://www.youtube.com/embed/WFm23haaWTQ\"></iframe>"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<iframe width=\"700\" height=\"400\" src=\"https://www.youtube.com/embed/WFm23haaWTQ\"></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvxcWO5pU9M3",
        "colab_type": "code",
        "outputId": "aae17fa6-577b-4234-c586-6cc967af2659",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        }
      },
      "source": [
        "%%HTML\n",
        "<iframe width=\"700\" height=\"400\" src=\"https://www.youtube.com/embed/Ujl8L4QoHHU\"></iframe>"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<iframe width=\"700\" height=\"400\" src=\"https://www.youtube.com/embed/Ujl8L4QoHHU\"></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDd75ij6SrK8",
        "colab_type": "text"
      },
      "source": [
        "# Drowsiness Detection\n",
        "\n",
        "## Feeding Eye aspect ratio for detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_RgaVLkTMMs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def eye_aspect_ratio(eye):\n",
        "\tA = distance.euclidean(eye[1], eye[5])\n",
        "\tB = distance.euclidean(eye[2], eye[4])\n",
        "\tC = distance.euclidean(eye[0], eye[3])\n",
        "\tear = (A + B) / (2.0 * C)\n",
        "\treturn ear\n",
        "\t\n",
        "thresh = 0.25\n",
        "frame_check = 20\n",
        "detect = dlib.get_frontal_face_detector()\n",
        "predict = dlib.shape_predictor(\"E:\\Github projects\\Drowsiness_Detection_fork\\shape_predictor_68_face_landmarks.dat\")# Dat file is the crux of the code\n",
        "\n",
        "(lStart, lEnd) = face_utils.FACIAL_LANDMARKS_68_IDXS[\"left_eye\"]\n",
        "(rStart, rEnd) = face_utils.FACIAL_LANDMARKS_68_IDXS[\"right_eye\"]\n",
        "cap=cv2.VideoCapture(0)\n",
        "flag=0\n",
        "while True:\n",
        "\tret, frame=cap.read()\n",
        "\tframe = imutils.resize(frame, width=450)\n",
        "\tgray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\tsubjects = detect(gray, 0)\n",
        "\tfor subject in subjects:\n",
        "\t\tshape = predict(gray, subject)\n",
        "\t\tshape = face_utils.shape_to_np(shape)#converting to NumPy Array\n",
        "\t\tleftEye = shape[lStart:lEnd]\n",
        "\t\trightEye = shape[rStart:rEnd]\n",
        "\t\tleftEAR = eye_aspect_ratio(leftEye)\n",
        "\t\trightEAR = eye_aspect_ratio(rightEye)\n",
        "\t\tear = (leftEAR + rightEAR) / 2.0\n",
        "\t\tleftEyeHull = cv2.convexHull(leftEye)\n",
        "\t\trightEyeHull = cv2.convexHull(rightEye)\n",
        "\t\tcv2.drawContours(frame, [leftEyeHull], -1, (0, 255, 0), 1)\n",
        "\t\tcv2.drawContours(frame, [rightEyeHull], -1, (0, 255, 0), 1)\n",
        "\t\tif ear < thresh:\n",
        "\t\t\tflag += 1\n",
        "\t\t\tprint (flag)\n",
        "\t\t\tif flag >= frame_check:\n",
        "\t\t\t\tcv2.putText(frame, \"****************ALERT!****************\", (10, 30),\n",
        "\t\t\t\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
        "\t\t\t\tcv2.putText(frame, \"****************ALERT!****************\", (10,325),\n",
        "\t\t\t\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
        "\t\t\t\t#print (\"Drowsy\")\n",
        "\t\telse:\n",
        "\t\t\tflag = 0\n",
        "\tcv2.imshow(\"Frame\", frame)\n",
        "\tkey = cv2.waitKey(1) & 0xFF\n",
        "\tif key == ord(\"q\"):\n",
        "break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PALmjFCSLN4",
        "colab_type": "code",
        "outputId": "9414e466-e306-45b4-c28a-fa5e22958c67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        }
      },
      "source": [
        "%%HTML\n",
        "<iframe width=\"700\" height=\"400\" src=\"https://www.youtube.com/embed/twmHZE20rRY\"></iframe>"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<iframe width=\"700\" height=\"400\" src=\"https://www.youtube.com/embed/twmHZE20rRY\"></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quZjhCWfTK_b",
        "colab_type": "code",
        "outputId": "b5e88a7f-f6e6-4163-ae80-9a81d41b55a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        }
      },
      "source": [
        "%%HTML\n",
        "<iframe width=\"700\" height=\"400\" src=\"https://www.youtube.com/embed/eFQvKHdjeEw\"></iframe>"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<iframe width=\"700\" height=\"400\" src=\"https://www.youtube.com/embed/eFQvKHdjeEw\"></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVexLdv3UwT9",
        "colab_type": "text"
      },
      "source": [
        "# Facial Recognition using FaceNets\n",
        "### For detailed code : https://github.com/akshaybahadur21/Facial-Recognition-using-Facenet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWYyudHBYKwp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def recognize_face(face_descriptor, database):\n",
        "    encoding = img_to_encoding(face_descriptor, FRmodel)\n",
        "    min_dist = 100\n",
        "    identity = None\n",
        "\n",
        "    # Loop over the database dictionary's names and encodings.\n",
        "    for (name, db_enc) in database.items():\n",
        "\n",
        "        # Compute L2 distance between the target \"encoding\" and the current \"emb\" from the database.\n",
        "        dist = np.linalg.norm(db_enc - encoding)\n",
        "\n",
        "        print('distance for %s is %s' % (name, dist))\n",
        "\n",
        "        # If this distance is less than the min_dist, then set min_dist to dist, and identity to name\n",
        "        if dist < min_dist:\n",
        "            min_dist = dist\n",
        "            identity = name\n",
        "\n",
        "    if int(identity) <=4:\n",
        "        return str('Akshay'), min_dist\n",
        "    if int(identity) <=8:\n",
        "        return str('Apoorva'), min_dist\n",
        "      \n",
        "def img_to_encoding(image, model):\n",
        "    image = cv2.resize(image, (96, 96)) \n",
        "    img = image[...,::-1]\n",
        "    img = np.around(np.transpose(img, (2,0,1))/255.0, decimals=12)\n",
        "    x_train = np.array([img])\n",
        "    embedding = model.predict(x_train)\n",
        "    return embedding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pin_fd1WT_Au",
        "colab_type": "code",
        "outputId": "4d220f01-b036-4c59-c223-fd387c018be7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        }
      },
      "source": [
        "%%HTML\n",
        "<iframe width=\"700\" height=\"400\" src=\"https://www.youtube.com/embed/v2dPVx9qCEo\"></iframe>"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<iframe width=\"700\" height=\"400\" src=\"https://www.youtube.com/embed/v2dPVx9qCEo\"></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuPVRH0wueLF",
        "colab_type": "text"
      },
      "source": [
        "# Open Pose"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48EQlmlNVdlt",
        "colab_type": "code",
        "outputId": "eace9bef-b560-4f33-f069-f46d25936d8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        }
      },
      "source": [
        "%%HTML\n",
        "<iframe width=\"700\" height=\"400\" src=\"https://www.youtube.com/embed/C1Sxk6zxWLM\"></iframe>"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<iframe width=\"700\" height=\"400\" src=\"https://www.youtube.com/embed/C1Sxk6zxWLM\"></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RK0Lk_-lujxa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "eaab1421-076f-47bd-f9b3-4030986aaeb5"
      },
      "source": [
        "%%HTML\n",
        "<iframe width=\"700\" height=\"400\" src=\"https://www.youtube.com/embed/xyiLxIMDiAY\"></iframe>"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<iframe width=\"700\" height=\"400\" src=\"https://www.youtube.com/embed/xyiLxIMDiAY\"></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R32-ZmXo2F1X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}